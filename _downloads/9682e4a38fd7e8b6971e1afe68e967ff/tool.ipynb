{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Tools\n\n## Background\n\nThere are two ways to call tools in LLM-empowered multi-agent applications.\n\n<img src=\"https://img.alicdn.com/imgextra/i3/O1CN01iizvjY1UjKCE3q5FR_!!6000000002553-0-tps-1830-792.jpg\" align=\"center\" alt=\"Two ways for tool calling\" width=\"80%\">\n\n- Prompt-based tool calling: Developers introduce tools in the prompt and extract tool calls from the LLM response.\n- API-based tool calling: Developers provide tools description in JSON schema format. The LLM API will directly return the tool calls in their specific format.\n\nAgentScope supports both ways. In this tutorial, we will introduce how to use\nthe built-in tools and how to create custom tools.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import json\n\nimport agentscope\nfrom agentscope.message import Msg\nfrom agentscope.models import DashScopeChatWrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using Built-in Tools\nAgentScope provides a `ServiceToolkit` module that supports to\n\n- parse tools into JSON schemas automatically\n- check arguments and call functions\n\nBefore using `ServiceToolkit`, we can take a look at the available tools in\nthe `agentscope.service` module.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from agentscope.service import get_help, ServiceResponse, ServiceExecStatus\n\nget_help()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All above functions are implemented as Python functions.\nThey can be registered to the `ServiceToolkit` by calling the `add` method.\nThe `ServiceToolkit` will parse the tool functions into JSON schema\nautomatically.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from agentscope.service import ServiceToolkit\nfrom agentscope.service import bing_search, execute_shell_command\n\ntoolkit = ServiceToolkit()\ntoolkit.add(execute_shell_command)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note some parameters of the tool functions (e.g. api_key) should be handled\nby developers.\nYou can directly pass these parameters as keyword arguments in the add\nmethod as follows, the reserved parameters will be left to the agent to fill.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "toolkit.add(bing_search, api_key=\"xxx\")\n\nprint(\"The tools instruction:\")\nprint(toolkit.tools_instruction)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The built-in default calling format:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(toolkit.tools_calling_format)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The JSON Schema description of the tool functions:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(json.dumps(toolkit.json_schemas, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt-based Tool Calling\nIn prompt-based tool calling, developers need to\n- introduce the tools and call format in prompt\n- parse and extract the tool calls from the LLM response.\n\nYou can use the parsers in `structured-output` section to parse the LLM\nresponse and extract the tool calls.\nThe tool call format of `ServiceToolkit` is as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from agentscope.message import ToolUseBlock\n\ntool_call = ToolUseBlock(\n    type=\"tool_use\",\n    id=\"xxx\",\n    name=\"bing_search\",\n    input={\"query\": \"AgentScope\"},\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After assembling the `ServiceToolkit`, you can integrate it into agent.\n\nIn AgentScope, we provide a `ReActAgent` to handle the tool usage, you can\ndirectly pass the `ServiceToolkit` object into this agent.\nRefer to `builtin-agent` for implementation details of this agent.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>`ReActAgent` constructs the prompt and parses the tools locally,</p></div>\n rather than through the tools API provided by the model API. For using the\n tools API, please refer to `tools-api`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from agentscope.agents import ReActAgent\n\nagentscope.init(\n    model_configs={\n        \"config_name\": \"my-qwen-max\",\n        \"model_type\": \"dashscope_chat\",\n        \"model_name\": \"qwen-max\",\n    },\n)\n\nagent = ReActAgent(\n    name=\"Friday\",\n    model_config_name=\"my-qwen-max\",\n    service_toolkit=toolkit,\n    sys_prompt=\"You're a helpful assistant named Friday.\",\n)\n\nmsg_task = Msg(\n    \"user\",\n    \"Help me to calculate 1615114134*4343434343\",\n    \"user\",\n)\n\nres = agent(msg_task)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## API-based Tool Calling\n\nIn API-based tool calling, developers only need to prepare the tools\ndescription in JSON schema format. However, different APIs differ in\n- the format of the tool description, and\n- how to construct the prompt with tool calls and execution results.\n\n<img src=\"https://img.alicdn.com/imgextra/i4/O1CN01HIHrnw21LrDxzrB4a_!!6000000006969-0-tps-1920-1080.jpg\" align=\"center\" alt=\"API-based tool calling\" width=\"100%\">\n\nThe above figure takes OpenAI as an example to show how API-based tool\ncalling works in AgentScope. We block API-specific requirements by\n`agentscope.formatter` and `ModelResponse` modules. All developers need to\nknow is\n\n1. `ServiceToolkit` will parse the tool functions into standard JSON schema automatically\n2. `Formatter` class will transform the JSON schemas and messages into the required format\n3. The tool calls are all unified into the same format (`ToolUseBlock`) within `ModelResponse`\n\n.. tip:: A new agent class `ReActAgentV2` is added for API-based tools calling!\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Currently, only the `format_chat` method supports tools API. The</p></div>\n `format_multi_agent` method will be supported in the future.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>API-based tool calling does not support streaming return yet, and</p></div>\n the related functionality is under development.\n\nHere we take DashScope as an example to show how to use the tools API.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from agentscope.formatters import DashScopeFormatter\nfrom agentscope.message import TextBlock, ToolUseBlock, ToolResultBlock\n\nmodel = DashScopeChatWrapper(\n    config_name=\"_\",\n    model_name=\"qwen-max\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 3 -> 4 in the figure, formating messages and JSON schemas:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "msgs = [\n    Msg(\"user\", \"Help me to execute shell cmd 'whoami'\", \"user\"),\n]\n\nformatted_msgs = DashScopeFormatter.format_chat(msgs)\nformatted_schemas = DashScopeFormatter.format_tools_json_schemas(\n    toolkit.json_schemas,\n)\nprint(json.dumps(formatted_msgs, indent=4, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 5 -> 6 -> 7 in the figure, getting the model response:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "response = model(formatted_msgs, tools=formatted_schemas)\nprint(\"tool_calls:\", json.dumps(response.tool_calls, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 8, creating a new message with the tool calls:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Create a new msg with the tool calls\ncontent = []\nif response.text:\n    content.append(TextBlock(type=\"text\", text=response.text))\nif response.tool_calls:\n    content.extend(response.tool_calls)\nmsgs.append(Msg(\"assistant\", content, \"assistant\", echo=True))\n\n# execute the tool calls\nmsg_execution = toolkit.parse_and_call_func(\n    response.tool_calls,\n    tools_api_mode=True,  # Must be ture for tools API\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Step 9, adding the execution results to the message list:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "msgs.append(msg_execution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's try to format the new message list with tool calls and result\nagain!\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "formatted_msgs = DashScopeFormatter.format_chat(msgs)\nprint(json.dumps(formatted_msgs, indent=4, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Up to now, we have already finished the API-based tool calling process.\nThe whole process refers to the implementation of\n`agentscope.agents.ReActAgentV2` class. You can also directly use this\nagent.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using MCP with ServiceToolkit\nAgentScope provides support for integrating MCP (Model Context Protocol)\nservers, enabling enhanced capabilities for models and tools. You can add\nMCP servers to the `ServiceToolkit` using the `add_mcp_servers` method,\nwhere you specify the configurations for each server.\nPlease note that MCP requires Python version >= 3.10.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "configs = {\n    \"mcpServers\": {\n        \"puppeteer\": {\n            \"command\": \"npx\",\n            \"args\": [\"-y\", \"@modelcontextprotocol/server-puppeteer\"],\n        },\n    },\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Add MCP server configurations to the ServiceToolkit\n`toolkit.add_mcp_servers(server_configs=configs)`\n\n## Creating Custom Tools\nA custom tool function must follow these rules:\n\n- Typing for arguments\n- Well-written docstring in Google style\n- The return of the function must be wrapped by `ServiceResponse`\n\nAfter calling the `toolkit.add` function, the tool function will be parsed\nautomatically and registered to the `ServiceToolkit`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def new_function(arg1: str, arg2: int) -> ServiceResponse:\n    \"\"\"A brief introduction of this function in one line.\n\n    Args:\n        arg1 (`str`):\n            Brief description of arg1\n        arg2 (`int`):\n            Brief description of arg2\n    \"\"\"\n    return ServiceResponse(\n        status=ServiceExecStatus.SUCCESS,\n        content=\"Done!\",\n    )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}