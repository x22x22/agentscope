{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Prompt Engineering\n\nThe prompt engineering is a crucial part of LLM-empowered applications,\nespecially for the multi-agent ones.\nHowever, most API providers focus on the chatting scenario, where a user and\nan assistant speak alternately.\n\nTo support multi-agent applications, AgentScope builds different prompt\nstrategies to convert a list of `Msg` objects to the required format.\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>There is no **one-size-fits-all** solution for prompt crafting.</p></div>\n The goal of built-in strategies is to **enable beginners to smoothly invoke\n the model API, rather than achieve the best performance**.\n For advanced users, we highly recommend developers to customize prompts\n according to their needs and model API requirements.\n\n## Using Built-in Strategy\n\nThe built-in prompt strategies are implemented in the `format` method of the\nmodel objects. Taking DashScope Chat API as an example:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from agentscope.models import DashScopeChatWrapper\nfrom agentscope.message import Msg\nimport json\n\n\nmodel = DashScopeChatWrapper(\n    config_name=\"_\",\n    model_name=\"qwen-max\",\n)\n\n# `Msg` objects or a list of `Msg` objects can be passed to the `format` method\nprompt = model.format(\n    Msg(\"system\", \"You're a helpful assistant.\", \"system\"),\n    [\n        Msg(\"assistant\", \"Hi!\", \"assistant\"),\n        Msg(\"user\", \"Nice to meet you!\", \"user\"),\n    ],\n)\n\nprint(json.dumps(prompt, indent=4, ensure_ascii=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After formatting the input messages, we can input the prompt into the model\nobject.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "response = model(prompt)\n\nprint(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non-Vision Models\n\nIn the following table, we list the built-in prompt strategies, as well as\nthe prefix of supported LLMs.\n\nTaking the following messages as an example:\n\n```python\nMsg(\"system\", \"You're a helpful assistant named Alice.\", \"system\"),\nMsg(\"Alice\", \"Hi!\", \"assistant\"),\nMsg(\"Bob\", \"Nice to meet you!\", \"user\")\n```\n.. list-table::\n    :header-rows: 1\n\n    * - LLMs\n      - `model_name`\n      - Constructed Prompt\n    * - OpenAI LLMs\n      - `gpt-`\n      -```python\n[\n    {\n        \"role\": \"system\",\n        \"name\": \"system\",\n        \"content\": \"You're a helpful assistant named Alice.\"\n    },\n    {\n        \"role\": \"user\",\n        \"name\": \"Alice\",\n        \"content\": \"Hi!\"\n    },\n    {\n        \"role\": \"user\",\n        \"name\": \"Bob\",\n        \"content\": \"Nice to meet you!\"\n    }\n]\n```\n    * - Gemini LLMs\n      - `gemini-`\n      -```python\n[\n    {\n        \"role\": \"user\",\n        \"parts\": [\n            \"You're a helpful assistant named Alice.\\\\n## Conversation History\\\\nAlice: Hi!\\\\nBob: Nice to meet you!\"\n        ]\n    }\n]\n```\n    * - All other LLMs\n\n        (e.g. DashScope, ZhipuAI ...)\n      -\n      -```python\n[\n    {\n        \"role\": \"system\",\n        \"content\": \"You're a helpful assistant named Alice.\"\n    },\n    {\n        \"role\": \"user\",\n        \"content\": \"## Conversation History\\\\nAlice: Hi!\\\\nBob: Nice to meet you!\"\n    }\n]\n```\n.. tip:: Considering some API libraries can support different LLMs (such as OpenAI Python library), AgentScope uses the `model_name` field to distinguish different models and decides the used strategy.\n\n## Vision Models\n\nFor vision models, AgentScope currently supports OpenAI vision models and\nDashscope multi modal API.\nThe more supported APIs will be added in the future.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}