{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# System Prompt Optimization\n\nAgentScope implements a module for optimizing Agent System Prompts.\n\n\n## System Prompt Generator\n\nThe system prompt generator uses a meta prompt to guide the LLM to generate\nthe system prompt according to the user's requirements, and allow the\ndevelopers to use built-in examples or provide their own examples as In\nContext Learning (ICL).\n\nThe system prompt generator includes a ``EnglishSystemPromptGenerator`` and a\n``ChineseSystemPromptGenerator`` module, which only differ in the used\nlanguage.\n\nWe take the ``EnglishSystemPromptGenerator`` as an example to illustrate how\nto use the system prompt generator.\n\n## Initialization\n\nTo initialize the generator, you need to first register your model\nconfigurations in the ``agentscope.init`` function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from agentscope.prompt import EnglishSystemPromptGenerator\nimport agentscope\n\nmodel_config = {\n    \"model_type\": \"dashscope_chat\",\n    \"config_name\": \"qwen_config\",\n    \"model_name\": \"qwen-max\",\n    # export your api key via environment variable\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The generator will use a built-in default meta prompt to guide the LLM to\ngenerate the system prompt.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "agentscope.init(\n    model_configs=model_config,\n)\n\nprompt_generator = EnglishSystemPromptGenerator(\n    model_config_name=\"qwen_config\",\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Users are welcome to freely try different optimization methods. We offer the\ncorresponding ``SystemPromptGeneratorBase`` module, which you can extend to\nimplement your own optimization module.\n\n## Generation\n\nCall the ``generate`` function of the generator to generate the system prompt\nas follows.\n\nYou can input a requirement, or your system prompt to be optimized.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "generated_system_prompt = prompt_generator.generate(\n    user_input=\"Generate a system prompt for a RED book (also known as Xiaohongshu) marketing expert, who is responsible for prompting books.\",\n)\n\nprint(generated_system_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generation with In Context Learning\n\nAgentScope supports in context learning in the system prompt generation.\n\nIt builds in a list of examples and allows users to provide their own\nexamples to optimize the system prompt.\n\nTo use examples, AgentScope provides the following parameters:\n\n- ``example_num``: The number of examples attached to the meta prompt, defaults to 0\n- ``example_selection_strategy``: The strategy for selecting examples, choosing from \"random\" and \"similarity\".\n- ``example_list``: A list of examples, where each example must be a dictionary with keys \"user_prompt\" and \"opt_prompt\". If not specified, the built-in example list will be used.\n\nNote, if you choose \"similarity\" as the example selection strategy, an\nembedding model could be specified in the ``embed_model_config_name`` or\n``local_embedding_model`` parameter.\n\nTheir differences are listed as follows:\n\n- ``embed_model_config_name``: You must first register the embedding model\nin ``agentscope.init`` and specify the model configuration name in this\nparameter.\n- ``local_embedding_model``: Optionally, you can use a local small embedding\nmodel supported by the ``sentence_transformers.SentenceTransformer`` library.\n\nAgentScope will use a default \"sentence-transformers/all-mpnet-base-v2\"\nmodel if you do not specify the above parameters, which is small enough to\nrun in CPU.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "icl_generator = EnglishSystemPromptGenerator(\n    model_config_name=\"qwen_config\",\n    example_num=3,\n    example_selection_strategy=\"random\",\n)\n\nicl_generated_system_prompt = icl_generator.generate(\n    user_input=\"Generate a system prompt for a RED book (also known as Xiaohongshu) marketing expert, who is responsible for prompting books.\",\n)\n\nprint(icl_generated_system_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>1. The example embeddings will be cached in ``~/.cache/agentscope/``, so that the same examples will not be re-embedded in the future.</p></div>\n 2. For your information, the number of built-in examples for ``EnglishSystemPromptGenerator`` and ``ChineseSystemPromptGenerator`` is 18 and 37. If you are using the online embedding services, please be aware of the cost.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}