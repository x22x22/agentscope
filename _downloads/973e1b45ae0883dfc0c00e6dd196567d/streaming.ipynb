{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Streaming Mode\n\nAgentScope supports streaming output for the following APIs in both terminal\nand AgentScope Studio.\n\n.. list-table::\n    :header-rows: 1\n\n    * - API\n      - Class\n      - Streaming\n    * - OpenAI Chat API\n      - `OpenAIChatWrapper`\n      - \u2713\n    * - DashScope Chat API\n      - `DashScopeChatWrapper`\n      - \u2713\n    * - Gemini Chat API\n      - `GeminiChatWrapper`\n      - \u2713\n    * - ZhipuAI Chat API\n      - `ZhipuAIChatWrapper`\n      - \u2713\n    * - Ollama Chat API\n      - `OllamaChatWrapper`\n      - \u2713\n    * - LiteLLM Chat API\n      - `LiteLLMChatWrapper`\n      - \u2713\n    * - Anthropic Chat API\n      - `AnthropicChatWrapper`\n      - \u2713\n\nThis section will show how to enable streaming mode in AgentScope and handle\nthe streaming response within an agent.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Enabling Streaming Output\n\nAgentScope supports streaming output by providing a `stream` parameter\nin model wrapper class.\nYou can directly specify the `stream` parameter in initialization or\nconfiguration.\n\n- Specifying in Initialization\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from agentscope.models import DashScopeChatWrapper\nimport os\n\nmodel = DashScopeChatWrapper(\n    config_name=\"_\",\n    model_name=\"qwen-max\",\n    api_key=os.environ[\"DASHSCOPE_API_KEY\"],\n    stream=True,  # Enabling the streaming output\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Specifying in Configuration\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model_config = {\n    \"model_type\": \"dashscope_chat\",\n    \"config_name\": \"qwen_config\",\n    \"model_name\": \"qwen-max\",\n    \"stream\": True,\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With the above configuration, we can obtain streaming output with built-in\nagents in AgentScope.\n\nNext, we show how to handle the streaming output within an agent.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Handling Streaming Response\n\nOnce we enable the streaming output, the returned model response will\ncontain a generator in its `stream` field.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prompt = [{\"role\": \"user\", \"content\": \"Hi!\"}]\n\nresponse = model(prompt)\nprint(\"The type of response.stream:\", type(response.stream))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can iterate over the generator to get the streaming text.\nA boolean value will also be yielded to indicate whether the current\nchunk is the last one.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for index, chunk in enumerate(response.stream):\n    print(f\"{index}.\", chunk)\n    print(f\"Current text field:\", response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>Note the generator is incremental and one-time.</p></div>\n\nDuring the iterating, the `text` field in the response will concatenate\nsub strings automatically.\n\nTo be compatible with non-streaming mode, you can also directly use\n`response.text` to obtain all text at once.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prompt = [{\"role\": \"user\", \"content\": \"Hi!\"}]\nresponse = model(prompt)\nprint(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Displaying Like Typewriter\nTo display the streaming text like a typewriter, AgentScope provides a\n`speak` function within the `AgentBase` class.\nIf a generator is given, the `speak` function will iterate over the\ngenerator and print the text like a typewriter in terminal or AgentScope\nStudio.\n\n```python\ndef reply(*args, **kwargs):\n    # ...\n    self.speak(response.stream)\n    # ...\n```\nTo be compatible with both streaming and non-streaming mode, we use the\nfollowing code snippet for all built-in agents in AgentScope.\n\n```python\ndef reply(*args, **kwargs):\n    # ...\n    self.speak(response.stream or response.text)\n    # ...\n```\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}