
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "build_tutorial/prompt.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_build_tutorial_prompt.py>`
        to download the full example code.

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_build_tutorial_prompt.py:


.. _prompt-engineering:

Prompt Engineering
================================

The prompt engineering is a crucial part of LLM-empowered applications,
especially for the multi-agent ones.
However, most API providers focus on the chatting scenario, where a user and
an assistant speak alternately.

To support multi-agent applications, AgentScope builds different prompt
strategies to convert a list of `Msg` objects to the required format.

.. note:: There is no **one-size-fits-all** solution for prompt crafting.
 The goal of built-in strategies is to **enable beginners to smoothly invoke
 the model API, rather than achieve the best performance**.
 For advanced users, we highly recommend developers to customize prompts
 according to their needs and model API requirements.

Using Built-in Strategy
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

The built-in prompt strategies are implemented in the `format` method of the
model objects. Taking DashScope Chat API as an example:

.. GENERATED FROM PYTHON SOURCE LINES 29-51

.. code-block:: Python


    from agentscope.models import DashScopeChatWrapper
    from agentscope.message import Msg
    import json


    model = DashScopeChatWrapper(
        config_name="_",
        model_name="qwen-max",
    )

    # `Msg` objects or a list of `Msg` objects can be passed to the `format` method
    prompt = model.format(
        Msg("system", "You're a helpful assistant.", "system"),
        [
            Msg("assistant", "Hi!", "assistant"),
            Msg("user", "Nice to meet you!", "user"),
        ],
    )

    print(json.dumps(prompt, indent=4, ensure_ascii=False))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    [
        {
            "role": "system",
            "content": "You're a helpful assistant."
        },
        {
            "role": "user",
            "content": "## Conversation History\nassistant: Hi!\nuser: Nice to meet you!"
        }
    ]




.. GENERATED FROM PYTHON SOURCE LINES 52-54

After formatting the input messages, we can input the prompt into the model
object.

.. GENERATED FROM PYTHON SOURCE LINES 54-59

.. code-block:: Python


    response = model(prompt)

    print(response.text)





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    Nice to meet you too! How can I assist you today?




.. GENERATED FROM PYTHON SOURCE LINES 60-139

Non-Vision Models
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

In the following table, we list the built-in prompt strategies, as well as
the prefix of supported LLMs.

Taking the following messages as an example:

.. code-block:: python

    Msg("system", "You're a helpful assistant named Alice.", "system"),
    Msg("Alice", "Hi!", "assistant"),
    Msg("Bob", "Nice to meet you!", "user")


.. list-table::
    :header-rows: 1

    * - LLMs
      - `model_name`
      - Constructed Prompt
    * - OpenAI LLMs
      - `gpt-`
      - .. code-block:: python

            [
                {
                    "role": "system",
                    "name": "system",
                    "content": "You're a helpful assistant named Alice."
                },
                {
                    "role": "user",
                    "name": "Alice",
                    "content": "Hi!"
                },
                {
                    "role": "user",
                    "name": "Bob",
                    "content": "Nice to meet you!"
                }
            ]
    * - Gemini LLMs
      - `gemini-`
      - .. code-block:: python

            [
                {
                    "role": "user",
                    "parts": [
                        "You're a helpful assistant named Alice.\\n## Conversation History\\nAlice: Hi!\\nBob: Nice to meet you!"
                    ]
                }
            ]
    * - All other LLMs

        (e.g. DashScope, ZhipuAI ...)
      -
      - .. code-block:: python

            [
                {
                    "role": "system",
                    "content": "You're a helpful assistant named Alice."
                },
                {
                    "role": "user",
                    "content": "## Conversation History\\nAlice: Hi!\\nBob: Nice to meet you!"
                }
            ]

.. tip:: Considering some API libraries can support different LLMs (such as OpenAI Python library), AgentScope uses the `model_name` field to distinguish different models and decides the used strategy.

Vision Models
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

For vision models, AgentScope currently supports OpenAI vision models and
Dashscope multi modal API.
The more supported APIs will be added in the future.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** (0 minutes 1.490 seconds)


.. _sphx_glr_download_build_tutorial_prompt.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: prompt.ipynb <prompt.ipynb>`

    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: prompt.py <prompt.py>`

    .. container:: sphx-glr-download sphx-glr-download-zip

      :download:`Download zipped: prompt.zip <prompt.zip>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
